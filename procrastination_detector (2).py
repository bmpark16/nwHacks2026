# -*- coding: utf-8 -*-
"""Procrastination_Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QqY4IXA4TQvTst_oXKJmYt82x17_O6D

# 1. Import and Install Dependencies
"""

!pip -q uninstall -y ydf tensorflow-decision-forests opentelemetry-proto grpcio-status mediapipe
!pip install mediapipe==0.10.14
!pip -q install -U "tensorflow==2.19.0" "keras==3.5.0"
!pip -q install -U opencv-python-headless scikit-learn matplotlib

import cv2
import numpy as np
import os
from matplotlib import pyplot as plt
import time
import mediapipe as mp

"""# 2. Keypoints using MP Holistic"""

mp_holistic = mp.solutions.holistic # Holistic model
mp_drawing = mp.solutions.drawing_utils # Drawing utilities

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB
    image.flags.writeable = False                  # Image is no longer writeable
    results = model.process(image)                 # Make prediction
    image.flags.writeable = True                   # Image is now writeable
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR
    return image, results

def draw_landmarks(image, results):
    # Updated to use correct MediaPipe constants
    # FACE_CONNECTIONS doesn't exist in mp_holistic, use FACEMESH_TESSELATION instead
    if results.face_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.face_landmarks,
            mp.solutions.face_mesh.FACEMESH_TESSELATION
        )

    if results.pose_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.pose_landmarks,
            mp_holistic.POSE_CONNECTIONS
        )

    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.left_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS
        )

    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.right_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS
        )

def draw_styled_landmarks(image, results):
    # Draw face landmarks (using FaceMesh constants)
    if results.face_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.face_landmarks,
            mp.solutions.face_mesh.FACEMESH_CONTOURS,
            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),
            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)
        )

    # Draw pose connections
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.pose_landmarks,
            mp_holistic.POSE_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),
            mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)
        )

    # Draw left hand connections
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.left_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),
            mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)
        )

    # Draw right hand connections
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            results.right_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS,
            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),
            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
        )

from google.colab import files
from google.colab.patches import cv2_imshow  # not used, but ok to keep
from IPython.display import HTML
import cv2
import os

# 1) Upload input video
uploaded = files.upload()
video_path = next(iter(uploaded))  # uploaded filename

# 2) Open input video
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise RuntimeError("Could not open uploaded video.")

# Read video properties
fps = cap.get(cv2.CAP_PROP_FPS)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Some videos may report fps as 0 in certain environments; set a fallback
if fps is None or fps == 0:
    fps = 30.0

# 3) Create output writer
out_path = "annotated_output.mp4"
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))

# 4) Process frames with MediaPipe and write annotated frames
with mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame is None:
            break

        # Make detections
        image, results = mediapipe_detection(frame, holistic)

        # Draw landmarks onto image
        draw_styled_landmarks(image, results)

        # Ensure frame is BGR for VideoWriter (your helper likely returns BGR already)
        # If your mediapipe_detection returns RGB, uncomment the next line:
        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        writer.write(image)

cap.release()
writer.release()

print("Saved:", out_path)

from base64 import b64encode

mp4 = open("annotated_output.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width=720 controls>
  <source src="{data_url}" type="video/mp4">
</video>
""")
files.download("annotated_output.mp4")

if results.left_hand_landmarks:
    print(len(results.left_hand_landmarks.landmark))
else:
    print("No left hand landmarks detected in the last processed frame.")

results

draw_landmarks(image, results)

plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

"""# 3. Extract Keypoint Values"""

len(results.left_hand_landmarks.landmark)

pose = []
for res in results.pose_landmarks.landmark:
    test = np.array([res.x, res.y, res.z, res.visibility])
    pose.append(test)

pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)
face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)
lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)

if results.face_landmarks :
  face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()
else :
  np.zeros(1404)

def extract_keypoints(results):
    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, face, lh, rh])

result_test = extract_keypoints(results)

result_test

468*3+33*4+21*3+21*3

np.save('0', result_test)

np.load('0.npy')

"""# 4. Setup Folders for Collection"""

# Path for exported data, numpy arrays
DATA_PATH = os.path.join('MP_Data')

# Actions that we try to detect
actions = np.array(['doomscrolling','nothing'])

# Thirty videos worth of data
no_sequences = 20

# Videos are going to be 30 frames in length
sequence_length = 150

for action in actions:
    for sequence in range(no_sequences):
        try:
            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))
        except:
            pass

"""# 5. Collect Keypoint Values for Training and Testing"""

from google.colab import drive
drive.mount('/content/drive')

VIDEOS_PATH = "MP_Data"          # change if your folder name differs
max_videos_per_class = None     # set to an int for quick testing, e.g. 10

# Make sure MP_Data/<action>/<sequence>/ exists
for action in actions:
    action_path = os.path.join(DATA_PATH, action)
    os.makedirs(action_path, exist_ok=True)
    print(action) #fleming

def list_video_files(folder):
    exts = (".mp4", ".mov", ".m4v", ".avi", ".mkv")
    files = [f for f in os.listdir(folder) if f.lower().endswith(exts)]
    files.sort()
    return files

def save_video_as_keypoints(video_path, action, sequence_idx, holistic, sequence_length):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"[WARN] Could not open: {video_path}")
        return False

    # Make sequence folder
    seq_dir = os.path.join(DATA_PATH, action, str(sequence_idx))
    os.makedirs(seq_dir, exist_ok=True)

    keypoints_list = []

    # --- Read up to sequence_length frames (TRUNCATE if longer) ---
    while len(keypoints_list) < sequence_length:
        ret, frame = cap.read()
        if not ret:
            break  # video ended (too short)
        image, results = mediapipe_detection(frame, holistic)
        keypoints_list.append(extract_keypoints(results))

    cap.release()

    if len(keypoints_list) == 0:
        print(f"[WARN] No frames read from: {video_path}")
        return False

    # --- PAD with zeros if too short ---
    feature_dim = keypoints_list[0].shape[0]
    while len(keypoints_list) < sequence_length:
        keypoints_list.append(np.zeros(feature_dim, dtype=np.float32))

    # --- Save as .npy files (same as original notebook) ---
    for frame_num, keypoints in enumerate(keypoints_list[:sequence_length]):
        np.save(os.path.join(seq_dir, str(frame_num)), keypoints)

    return True

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    for action in actions:
        action_video_dir = os.path.join(VIDEOS_PATH, action)
        if not os.path.isdir(action_video_dir):
            raise FileNotFoundError(f"Missing folder: {action_video_dir}")

        video_files = list_video_files(action_video_dir)
        if max_videos_per_class is not None:
            video_files = video_files[:max_videos_per_class]

        print(f"\nProcessing {len(video_files)} videos for class '{action}'...")

        for sequence_idx, vf in enumerate(video_files):
            video_path = os.path.join(action_video_dir, vf)
            ok = save_video_as_keypoints(video_path, action, sequence_idx, holistic, sequence_length)
            if ok:
                print(f"  ✓ {vf} -> MP_Data/{action}/{sequence_idx}/[0..{sequence_length-1}].npy")
            else:
                print(f"  ✗ {vf} skipped")

"""# 6. Preprocess Data and Create Labels and Features"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

label_map = {label:num for num, label in enumerate(actions)}
print(actions)

label_map

sequences, labels = [], []

for action in actions:
    action_path = os.path.join(DATA_PATH, action)

    # Get only numeric folders (0, 1, 2, ...)
    sequence_folders = sorted(
        [f for f in os.listdir(action_path) if f.isdigit()],
        key=int
    )

    for folder in sequence_folders:
        folder_path = os.path.join(action_path, folder)

        # Load all frame npys inside this folder
        frame_files = sorted(
            [f for f in os.listdir(folder_path) if f.endswith('.npy')],
            key=lambda x: int(x.split('.')[0])
        )

        # Safety check
        if len(frame_files) != sequence_length:
            continue

        window = []
        for frame_file in frame_files:
            res = np.load(os.path.join(folder_path, frame_file))
            window.append(res)

        sequences.append(window)
        labels.append(label_map[action])

np.array(sequences).shape

np.array(labels).shape

X = np.array(sequences)

X.shape

y = to_categorical(labels).astype(int)

y

# Normalize the data
X_mean = X.mean(axis=(0, 1), keepdims=True)
X_std = X.std(axis=(0, 1), keepdims=True)
X_normalized = (X - X_mean) / (X_std + 1e-8)  # Add epsilon to avoid division by zero

# Use normalized data for train/test split
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.05)

y_test.shape

# ========================================
# DATA VALIDATION - Run this to check for bad data/labels
# ========================================

print("=" * 60)
print("DATA VALIDATION REPORT")
print("=" * 60)

# 1. Check basic shape consistency
print("\n1. SHAPE VALIDATION")
print(f"   Total sequences: {len(sequences)}")
print(f"   Total labels: {len(labels)}")
print(f"   Expected feature dimension: 1662")
print(f"   Expected sequence length: {sequence_length}")

assert len(sequences) == len(labels), "ERROR: Mismatch between sequences and labels!"
print("   ✓ Sequences and labels count match")

# 2. Validate each sequence shape and data quality
print("\n2. SEQUENCE VALIDATION")
bad_sequences = []
for idx, seq in enumerate(sequences):
    seq_array = np.array(seq)

    # Check shape
    if seq_array.shape != (sequence_length, 1662):
        bad_sequences.append((idx, f"Bad shape: {seq_array.shape}"))
        continue

    # Check for NaN or Inf values
    if np.isnan(seq_array).any():
        bad_sequences.append((idx, "Contains NaN values"))
        continue

    if np.isinf(seq_array).any():
        bad_sequences.append((idx, "Contains Inf values"))
        continue

    # Check if all zeros (likely failed detection)
    if np.all(seq_array == 0):
        bad_sequences.append((idx, "All zeros - no detection"))
        continue

    # Check for unrealistic coordinate values (should be normalized 0-1 range mostly)
    if np.any(seq_array > 100) or np.any(seq_array < -100):
        bad_sequences.append((idx, f"Extreme values detected: min={seq_array.min():.2f}, max={seq_array.max():.2f}"))

if bad_sequences:
    print(f"   ✗ FOUND {len(bad_sequences)} BAD SEQUENCES:")
    for idx, reason in bad_sequences[:10]:  # Show first 10
        print(f"     - Sequence {idx}: {reason}")
    if len(bad_sequences) > 10:
        print(f"     ... and {len(bad_sequences) - 10} more")
else:
    print("   ✓ All sequences have valid shapes and values")

# 3. Label validation
print("\n3. LABEL VALIDATION")
unique_labels = np.unique(labels)
print(f"   Unique labels found: {unique_labels}")
print(f"   Expected labels: {list(label_map.values())}")

# Check label distribution
for action_name, label_idx in label_map.items():
    count = labels.count(label_idx)
    print(f"   - '{action_name}' (label {label_idx}): {count} samples")

# Check for invalid labels
valid_label_set = set(label_map.values())
invalid_labels = [l for l in labels if l not in valid_label_set]
if invalid_labels:
    print(f"   ✗ FOUND {len(invalid_labels)} INVALID LABELS: {set(invalid_labels)}")
else:
    print("   ✓ All labels are valid")

# 4. Class balance check
print("\n4. CLASS BALANCE")
label_counts = {label: labels.count(label) for label in unique_labels}
max_count = max(label_counts.values())
min_count = min(label_counts.values())
imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')

print(f"   Max samples per class: {max_count}")
print(f"   Min samples per class: {min_count}")
print(f"   Imbalance ratio: {imbalance_ratio:.2f}x")

if imbalance_ratio > 3:
    print("   ⚠ WARNING: Significant class imbalance detected!")
    print("     Consider collecting more data for underrepresented classes")
else:
    print("   ✓ Classes are reasonably balanced")

# 5. Statistical checks on keypoint data
print("\n5. STATISTICAL VALIDATION")
all_data = np.array(sequences)
print(f"   Data shape: {all_data.shape}")
print(f"   Data type: {all_data.dtype}")
print(f"   Mean: {all_data.mean():.4f}")
print(f"   Std: {all_data.std():.4f}")
print(f"   Min: {all_data.min():.4f}")
print(f"   Max: {all_data.max():.4f}")

# Check for data leakage (duplicate sequences)
print("\n6. DUPLICATE DETECTION")
unique_seqs = []
duplicate_indices = []
for idx, seq in enumerate(sequences):
    seq_hash = hash(np.array(seq).tobytes())
    if seq_hash in unique_seqs:
        duplicate_indices.append(idx)
    else:
        unique_seqs.append(seq_hash)

if duplicate_indices:
    print(f"   ⚠ WARNING: Found {len(duplicate_indices)} potential duplicate sequences")
    print(f"     Indices: {duplicate_indices[:10]}")
else:
    print("   ✓ No duplicate sequences detected")

# 7. Per-class quality check
print("\n7. PER-CLASS QUALITY")
for action_name, label_idx in label_map.items():
    class_sequences = [sequences[i] for i, l in enumerate(labels) if l == label_idx]
    if class_sequences:
        class_data = np.array(class_sequences)
        zeros_ratio = (class_data == 0).sum() / class_data.size
        print(f"   {action_name}:")
        print(f"     - Samples: {len(class_sequences)}")
        print(f"     - Zero ratio: {zeros_ratio:.2%}")
        print(f"     - Mean: {class_data.mean():.4f}, Std: {class_data.std():.4f}")

        if zeros_ratio > 0.5:
            print(f"     ⚠ WARNING: High zero ratio - poor landmark detection!")

# Final summary
print("\n" + "=" * 60)
print("VALIDATION SUMMARY")
print("=" * 60)
critical_issues = len([b for b in bad_sequences if "NaN" in b[1] or "Inf" in b[1]])
warnings = len(bad_sequences) - critical_issues

if critical_issues > 0:
    print(f"✗ CRITICAL: {critical_issues} sequences with NaN/Inf values")
    print("  ACTION REQUIRED: Remove or fix these sequences before training!")
elif warnings > 0:
    print(f"⚠ WARNING: {warnings} sequences with quality issues")
    print("  RECOMMENDED: Review and consider removing these sequences")
else:
    print("✓ All validation checks passed!")
    print("  Data is ready for training")

print("=" * 60)

"""# 7. Build and Train LSTM Neural Network"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import TensorBoard

log_dir = os.path.join('Logs')
tb_callback = TensorBoard(log_dir=log_dir)

model = Sequential()
model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(150,1662)))
model.add(LSTM(32, return_sequences=False, activation='tanh'))
model.add(Dense(32, activation='tanh'))
model.add(Dense(actions.shape[0], activation='softmax'))

res = [.7, 0.2, 0.1]

actions[np.argmax(res)]

model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model.fit(X_train, y_train, epochs=50, callbacks=[tb_callback])

model.summary()

"""# 8. Make Predictions"""

res = model.predict(X_test)

actions[np.argmax(res[0])]

actions[np.argmax(y_test[0])]

"""# 9. Save Weights"""

model.save('action.h5')

model.load_weights('action.h5')

"""# 10. Evaluation using Confusion Matrix and Accuracy"""

from sklearn.metrics import multilabel_confusion_matrix, accuracy_score

yhat = model.predict(X_test)

ytrue = np.argmax(y_test, axis=1).tolist()
yhat = np.argmax(yhat, axis=1).tolist()

multilabel_confusion_matrix(ytrue, yhat)

accuracy_score(ytrue, yhat)

